{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aadhaar Data Cleaning and Preprocessing\n",
    "\n",
    "This notebook performs comprehensive data cleaning and preprocessing for all three Aadhaar datasets:\n",
    "- Enrolment dataset\n",
    "- Demographic updates dataset\n",
    "- Biometric updates dataset\n",
    "\n",
    "**Objectives:**\n",
    "1. Load all datasets\n",
    "2. Standardize column names and date formats\n",
    "3. Handle missing values\n",
    "4. Aggregate data by date, state, and district\n",
    "5. Create unified analytical tables\n",
    "6. Generate data quality report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Import custom utilities\n",
    "from utils.preprocessing import (\n",
    "    load_datasets,\n",
    "    standardize_columns,\n",
    "    handle_missing_values,\n",
    "    aggregate_data,\n",
    "    create_unified_table,\n",
    "    get_data_summary,\n",
    "    save_cleaned_data,\n",
    "    preprocess_all_datasets\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Raw Datasets\n",
    "\n",
    "Load all three CSV files and perform initial inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "enrolment_df, demographic_df, biometric_df = load_datasets('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display enrolment dataset info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ENROLMENT DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nShape: {enrolment_df.shape}\")\n",
    "print(f\"\\nColumns: {list(enrolment_df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(enrolment_df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(enrolment_df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(enrolment_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display demographic dataset info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DEMOGRAPHIC UPDATE DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nShape: {demographic_df.shape}\")\n",
    "print(f\"\\nColumns: {list(demographic_df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(demographic_df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(demographic_df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(demographic_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display biometric dataset info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BIOMETRIC UPDATE DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nShape: {biometric_df.shape}\")\n",
    "print(f\"\\nColumns: {list(biometric_df.columns)}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "display(biometric_df.head())\n",
    "print(f\"\\nData types:\")\n",
    "print(biometric_df.dtypes)\n",
    "print(f\"\\nMissing values:\")\n",
    "print(biometric_df.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Run Complete Preprocessing Pipeline\n",
    "\n",
    "Execute the automated preprocessing pipeline that:\n",
    "- Standardizes all column names\n",
    "- Handles missing values\n",
    "- Aggregates data\n",
    "- Creates unified table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete preprocessing\n",
    "enrolment_clean, demographic_clean, biometric_clean, unified_df = preprocess_all_datasets('../data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "Assess the quality of cleaned data and visualize key statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries for all datasets\n",
    "summaries = []\n",
    "for df, name in [(enrolment_clean, \"Enrolment\"),\n",
    "                 (demographic_clean, \"Demographic\"),\n",
    "                 (biometric_clean, \"Biometric\"),\n",
    "                 (unified_df, \"Unified\")]:\n",
    "    summary = get_data_summary(df, name)\n",
    "    summaries.append(summary)\n",
    "\n",
    "# Create summary dataframe\n",
    "summary_df = pd.DataFrame(summaries)\n",
    "print(\"\\nDATA SUMMARY:\")\n",
    "display(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset sizes\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Record counts\n",
    "axes[0].bar(summary_df['name'], summary_df['rows'], color='skyblue', edgecolor='navy')\n",
    "axes[0].set_title('Dataset Sizes (Number of Records)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Dataset')\n",
    "axes[0].set_ylabel('Number of Records')\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Geographic coverage\n",
    "geo_data = summary_df[['name', 'states', 'districts']].dropna()\n",
    "x = np.arange(len(geo_data))\n",
    "width = 0.35\n",
    "axes[1].bar(x - width/2, geo_data['states'], width, label='States', color='coral', edgecolor='darkred')\n",
    "axes[1].bar(x + width/2, geo_data['districts'], width, label='Districts', color='lightgreen', edgecolor='darkgreen')\n",
    "axes[1].set_title('Geographic Coverage', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Dataset')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels(geo_data['name'], rotation=45)\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/data_quality_overview.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved visualization to outputs/data_quality_overview.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Temporal Coverage Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze temporal coverage\n",
    "print(\"\\nTEMPORAL COVERAGE:\")\n",
    "print(\"=\"*60)\n",
    "for df, name in [(enrolment_clean, \"Enrolment\"),\n",
    "                 (demographic_clean, \"Demographic\"),\n",
    "                 (biometric_clean, \"Biometric\")]:\n",
    "    if 'date' in df.columns:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Start date: {df['date'].min()}\")\n",
    "        print(f\"  End date: {df['date'].max()}\")\n",
    "        print(f\"  Duration: {(df['date'].max() - df['date'].min()).days} days\")\n",
    "        print(f\"  Unique dates: {df['date'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize temporal distribution\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10))\n",
    "\n",
    "datasets = [(enrolment_clean, \"Enrolment\", 'blue'),\n",
    "            (demographic_clean, \"Demographic Updates\", 'green'),\n",
    "            (biometric_clean, \"Biometric Updates\", 'red')]\n",
    "\n",
    "for idx, (df, name, color) in enumerate(datasets):\n",
    "    if 'date' in df.columns:\n",
    "        # Aggregate by date\n",
    "        daily_counts = df.groupby('date').size()\n",
    "        \n",
    "        axes[idx].plot(daily_counts.index, daily_counts.values, color=color, linewidth=2)\n",
    "        axes[idx].fill_between(daily_counts.index, daily_counts.values, alpha=0.3, color=color)\n",
    "        axes[idx].set_title(f'{name} - Temporal Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[idx].set_ylabel('Number of Records')\n",
    "        axes[idx].grid(True, alpha=0.3)\n",
    "        \n",
    "axes[2].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../outputs/temporal_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"✓ Saved visualization to outputs/temporal_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Cleaned Datasets\n",
    "\n",
    "Save all cleaned datasets for use in subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create processed data directory\n",
    "Path('../data/processed').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save cleaned datasets\n",
    "save_cleaned_data(enrolment_clean, 'enrolment_clean.csv', '../data/processed')\n",
    "save_cleaned_data(demographic_clean, 'demographic_clean.csv', '../data/processed')\n",
    "save_cleaned_data(biometric_clean, 'biometric_clean.csv', '../data/processed')\n",
    "save_cleaned_data(unified_df, 'unified_clean.csv', '../data/processed')\n",
    "\n",
    "print(\"\\n✓ All datasets saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
metadata": {},
   "source": [
    "## 6. Data Quality Report Summary\n",
    "\n",
    "Final summary of data quality and preprocessing results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DATA QUALITY REPORT - SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n✓ PREPROCESSING COMPLETED SUCCESSFULLY\\n\")\n",
    "\n",
    "print(\"Datasets Processed:\")\n",
    "print(\"  1. Enrolment dataset\")\n",
    "print(\"  2. Demographic updates dataset\")\n",
    "print(\"  3. Biometric updates dataset\")\n",
    "print(\"  4. Unified analytical table\")\n",
    "\n",
    "print(\"\\nCleaning Steps Applied:\")\n",
    "print(\"  ✓ Column name standardization\")\n",
    "print(\"  ✓ Date format standardization\")\n",
    "print(\"  ✓ Missing value handling\")\n",
    "print(\"  ✓ Geographic data cleaning\")\n",
    "print(\"  ✓ Data aggregation by date/state/district\")\n",
    "\n",
    "print(\"\\nData Quality Metrics:\")\n",
    "print(f\"  • Total records (unified): {len(unified_df):,}\")\n",
    "print(f\"  • States covered: {unified_df['state'].nunique()}\")\n",
    "print(f\"  • Districts covered: {unified_df['district'].nunique()}\")\n",
    "print(f\"  • Missing values (unified): {unified_df.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Ready for Exploratory Data Analysis!\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
